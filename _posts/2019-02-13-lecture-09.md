--- 
layout: distill
title: Lecture Notes Template
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-01-09

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Wenwen Si # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Siyu Gao
    url: "#"
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
  
---

# Lecture 09 Modeling Networks

## Network research - study at graph as object

Do graphs and networks really exsit in real world? No.
Where fo networks come from? Different rule for define the graph will generate different graph.
none of the network exsits physically

## Structural Learning
### Trees: The Chow-Liu algorithm
Directly search for optimal tree structure
### Pairwise Markov Random Fields
<figure>
  <img src="{{ '/assets/img/notes/lecture-09/node4network.png' | relative_url }}" />
</figure>
- Key idea:  network inference as parameter estimation
- every node: observations can include binary / continuous number
- vairables connected pairwise (pairwise MRF, also BM)

$$ p(x_1, x_2, x_3, x_4) = \frac{1}{Z} exp\{\theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4  +  \theta_{12}x_1x_2 + \theta_{13}x_1x_3 + \theta_{23}x_2x_3 + \theta_{34}x_3x_4 \} $$

- use non-zero parameters to represent edges, turn topology into continuous space
<figure>
  <img src="{{ '/assets/img/notes/lecture-09/matrix2topo.png' | relative_url }}" />
</figure>

- Model
  - discrete nodal states (Ising/Potts model)
  - continuous (Gaussian grahical model)
  - heterogeneous
- parameter matrix encodes graph structure (non zero $\iff$ edge)

### Multivariant Gaussian 

  $$p(\vec x \mid \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} exp\{-\frac{1}{2}(\vec x - \mu)^T \Sigma^{-1} (\vec x - \mu) \} $$

  Let $\mu = 0, Q = \Sigma^{-1}$, we have

  $$p(x_1,x_2,...,x_p|\mu = 0,Q) = \frac{|Q|^\frac{1}{2}}{(2\pi)^{\frac{n}{2}}} exp \{ -\frac{1}{2} \sum_{i}q_{ii}(x_i)^2 - \sum_{i < j} q_{ij}x_ix_j \} $$

  - The equation can viewed as a continuous Markov Random Field with potentials defined on every node and edge
  - $q_{ii}(x_i)^2$ can be treated as the node potential
  - $q_{ij}x_ix_j$ can be treated as the edge potential

### Gaussian Graphical Model

  $$ \textbf{X}^{(n)} \sim \mathcal{N}(\textbf{0}, \Sigma^{(n)}) $$

  where $\Sigma^{(n)}$ encode dependencies among variables. As we mentioned before, the inversed matrix of covariance matrix is called precision matrix which encodes non-zero edges in Gaussian Graphical Model

### Markov vs Correlation Network
  
  Markov fits more on real world because it can model conditioncal probabilities
#### Correlation network 
    
  $$\Sigma_{i,j} = 0 \Rightarrow X_i \perp X_j \quad or \quad p(X_i, X_j) = p(X_i)p(X_j)$$

  - Correlation network is based on covariance matrix $\Sigma$

  - So $\Sigma_{i,j} = 0$ means that $X_i$ and $X_j$ are marginally independent without observing other variables 
  - In fact, this kind of independence is hard to find in the real world problems

#### Gaussian Graphical Model
   
  $$Q_{i,j}=0 \Rightarrow X_i \perp X_j \mid X_{-ij} \quad or  \quad p(X_i, X_j \mid X_{-ij}) = p(X_i \mid X_{-ij})p(X_j \perp X_{-ij})$$

  - A Gaussian Graphical Model (GGM) is a Markov Network based on precision matrix $Q$ 
  - Conditional Independence/Partial Correlation Coefficients are a more sophisticated dependence measure
  - This kind of independence is more suitable for modeling the real world situations
<figure>
  <img src="{{ '/assets/img/notes/lecture-09/gaussian.png' | relative_url }}" />
</figure>

  - Problem
    - You want the precison matrix to be invertible, but non full rank does not mean MN does not exist
    - The computationla complexity of inversing a matrix is $O(N^3)$ where N is the number of node in a graph
    - With small sample size, empirical convariance matrix cannot be inverted
    - sparsity: inversion of matrix not scalable

  Now our purpose is to obtain the precision matrix, so we need to make a assumption to avoid these problems.

### Prior Assumption of GGM - Sparsity

  - Common assumption to make: percision matrix $Q$ is spare
  - Makes empirical sense: Genes are only assumed to interface with small group of other genes
  - Makes statistical sense: Learing is feasible in high dimensions with small sample siz

  After making this assumption, it is still hard to get the whole precision matrix. What if we get the precision matrix row by row or column by column. Then it came to the basic method to make this simpler thought possible.

### GGM with Lasso

  Our target is to select the neighborhodd of each node so we can perform regression of all nodes to express the relationship between two nodes according to the parameters in the regression. However, there will be a problem that as long as we use the normal regression to one node, other nodes in the network would be the neighborhood of this node since the parameters are non-zero. That is the reason why we need LASSO regression, which can cause zero parameters through penalty. Therefore, the regression problem at each node can be formulated the following LASSO problem:

  $$\hat{\beta_{1}} = \arg\min_{\beta_{1}} \|\textbf{Y} - \textbf{X}\beta_{1}\|^{2} + \lambda\|\beta_{1}\|_{1}$$ 

  where $\textbf{Y}$ is an N dimensional vector with each entry being a node at a specific dimension from the N sets of data observations. Each set of data observation has P nodes, which is illustrated as the P nodes forming a circle in the following fighures. 
<figure>
  <img src="{{ '/assets/img/notes/lecture-09/lasso_exp.png' | relative_url }}" />
</figure>
  $\textbf{X}$ is an $N \times (P −1)$ dimensional matrix with each column being the rest of the nodes in the N sets of observations. $\beta_1$ is a P − 1 dimensional coefficient weight vector one wants to estimate. The last $l_1$ term in the equation is the sparse promoting penalty term that enforces sparsity. Notice ideally we want to use $\| \beta_1 \|_{0}$ which is the $l_0$ term to minimize the number of supports. However this makes the problem non-convex and difficult to solve. One therefore relaxes this problem to the $l_1$ case. **The above regression is repeated for each node and the zero valued edges are removed.** Generally, one is more interested in whether pairwise nodes are conditionally independent or not.

  Only when the following assumptions are met, LASSO regression can asymptotically recover correct subset of covariates that relevant. 
  - **Dependency Condition**: Relevant Covariates are not overly dependent
  - **Incoherence Condition**: Large number of irrelevant covariates cannot be too correlated with relevant covariates
  - **Strong concentration bounds**: Sample quantities converge to expected values quickly

  And theoretically, there have been proof that graphical lasso can recover the true structure of the graph. [Meinshausen et al.][1] proved that if:

  $$\lambda_s > C\sqrt{\frac{logp}{S}}$$

  then with high possibility,

  $$S(\hat{\beta}) \to S(\beta_{\ast})$$


  [1]: https://projecteuclid.org/euclid.aos/1232115934

### Why this algorithm work?

  After knowing the process of this algorithm to obtain the precision matrix through repeatedly applying LASSO regression on each node, we want to know why we can get the precision matrix in this way. In other words, we want to know why LASSO regression can select the neighborhood of each node. 

#### Multivariate Gaussian
  
  For the multivatiate Gaussianm there are several formulas to remember

  $$p(x_2) = \mathcal{N}(x_2 | \mu_2, \Sigma_{22}) $$

  $$p(x_1|x_2) = \mathcal{N}(x_2 | \mu_{1|2}, V_{1|2}) $$
  
  where we have 
  $ \mu_{1|2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2 - \mu_2) $
  and 
  $ V_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} $

#### The matrix inverse lemma

  Consider a block-partitioned matrix:

  $$
  M=
   \left[
   \begin{matrix}
     E & F \\
     G & H \\
     \end{matrix}
    \right] 
  $$

  We diagonalize $M$ firstly

  $$
   \left[
   \begin{matrix}
     I & -FH^{-1} \\
     0 & I \\
     \end{matrix}
    \right] 
    \left[
   \begin{matrix}
     I & 0 \\
     G & H \\
     \end{matrix}
    \right] 
    \left[
    \begin{matrix}
     I & 0 \\
     -H^{-1}G & I \\
     \end{matrix}
    \right] 
    =
    \left[
   \begin{matrix}
     E-FH^{-1}G & 0 \\
     0 & H \\
     \end{matrix}
    \right] 
  $$

  According to the Schur complement 
  $M/H = E-FH^{-1}G$
  , then we use this formula 
  $XYZ = W \Rightarrow Y^{-1} = ZW^{-1}X$
  to inverse

  $$
    M=
   \left[
   \begin{matrix}
     E & F \\
     G & H \\
     \end{matrix}
    \right]^{-1} =
    \left[
   \begin{matrix}
     I & 0 \\
     -H^{-1}G & I \\
     \end{matrix}
    \right] 
    \left[
    \begin{matrix}
     (M/H)^{-1} & 0 \\
     0 & H^{-1} \\
     \end{matrix}
    \right] 
    \left[
   \begin{matrix}
     I & -FH^{-1} \\
     0 & I \\
     \end{matrix}
    \right] =
    \left[
   \begin{matrix}
     (M/H)^{-1} & -(M/H)^{-1}FH^{-1} \\
     -H^{-1}G(M/H)^{-1} & H^{-1} +H^{-1}G(M/H)^{-1}FH^{-1} \\
     \end{matrix}
    \right] 

  $$

  The matrix inverse lemma is 

  $$(E-FH^{-1}G)^{-1} = E^{-1} +E^{-1}F(H-GE^{-1}F)^{-1}GE^{-1}$$

#### The covariance and the precision matrices

  If we have the covariance matrix 

  $$
  \Sigma=
   \left[
   \begin{matrix}
     \sigma_{11} & \vec{\sigma}_1^{T} \\
     \vec {\sigma}_1 & \Sigma_{-1} \\
     \end{matrix}
    \right] 
  $$

  Also recall the facts about matrix inverse derived in the previous section, we can have the precision matrix:

  $$
  Q=
   \left[
   \begin{matrix}
     q_{11} & -q_{11}\vec{\sigma}_1^{T}\Sigma_{-1}^{-1} \\
     -q_11\Sigma_{-1}^{-1}\vec {\sigma}_1 & \Sigma_{-1}^{-1}(I+q_{11}\vec {\sigma}_1\vec{\sigma}_1^{T}) \\
     \end{matrix}
    \right] 
    =
   \left[
   \begin{matrix}
     q_{11} & \vec q_1^T \\
     \vec q_1& Q_{-1} \\
     \end{matrix}
    \right] 
  $$

#### Justification
  With the above three facts, one is ready to justify why the problem can be formulated as a LASSO variable selection problem. Given a Gaussian distribution, the conditional distribution of a single node i given the rest of the nodes can be written as:

  $$p(X_i|\textbf{X}_{-i}) = \mathcal{N}(\mu_i+\Sigma_{X_i\textbf{X}_{-i}}\Sigma_{\textbf{X}_{-i}\textbf{X}_{-i}}^{-1}(\textbf{X}_{-i}-\mu_{\textbf{X}_{-i}}),\Sigma_{X_iX_i} - \Sigma_{X_i\textbf{X}_{-i}}\Sigma_{\textbf{X}_{-i}\textbf{X}_{-i}}^{-1}\Sigma_{\textbf{X}_{-i}X_i})$$

  Let 
  $\mu = 0$
  we have 

  $$p(X_i|\textbf{X}_{-i}) = \mathcal{N}(\frac{\vec q_i}{-q_{ii}}\textbf{X}_{-i}, q_{i|-i})$$

  From here we can already see that the value of a certain node is determined by the linear representation of the other nodes plus a Gaussian noise:

  $$X_i \gets \beta_i\textbf{X}_{-i}+Noise(Gaussian)$$

  Neighborhood estimation based on auto-regression coeeficient

  $$S_i \equiv \{ j:j \not = i, \theta_{ij} \not = 0\}$$

  If we are given the estimation of the neighborhood 
  $S_i$ 
  we have 

  $$p(X_i|\textbf{X}_{-i}) = p(X_i|\textbf{X}_{S})$$

  Therefore, the neighborhood 
  $S_i$
  defines the Markov blanket of node 
  $i$
  . That is the reason why we can use LASSO regression to each node in order to get the precision matrix.
